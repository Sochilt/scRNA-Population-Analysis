# scRNA-Population-Analysis
The purpose of this analysis is to determine what populations are present, specifically in a microenvironment for scRNA-seq data.
The code is based around Seurat with addtional computational analysis for deeper insights. File used for tertiary scRNA analysis is a filtered feature_bc_matrix file ('bc' stands for barcode). 
This file is a sparse counts matrix file of gene expression data of a subset of barcodes identified as corresponding to actual cells.
This is the file that stores gene expression counts based on UMIs (unique molecular identifiers).
Go to https://satijalab.org/seurat/articles/pbmc3k_tutorial.html for the preliminary QC, pre-procossing and clustering code/information.
scType for automated annotation of the clusters found below is offered by IanevskiAleksandr lab at https://github.com/IanevskiAleksandr/sc-type

![UMAP plot](/Images/Annotated_Clusters.png)

```r
# Call in necessary libraries that provide pre-written, reusable functions and tools to do job.
library(dplyr)
library(Seurat)
library(patchwork)
library(readr)
library(rhdf5)
```
Ensure you set your working directory to where you have the "filtered_feature_bc_matrix" files stored.

```r
# Set working directory to where your files are. Be sure to include " " in the code below. 'data' now becomes that pathway. Do this for each matrix file (data, data1, data2...)
setwd("/home/user/documents")
data <- "/home/user/documents/sampleID/filtered_feature_bc_matrix"
```

The filtered_feature_bc_matrix contains files like barcodes.tsv.gz, features.tsv.gz, and matrix.mtx.gz, generated by 10X Genomics' Cell Ranger pipeline.
Read this pathway in.  I usually name the new directory as the sample ID.
```r
# Example
CLL1234 <- Read10X(data)
CLL5678 <- Read10X(data1)

# If your file is an .h5 file then use Read10X_h5. Ex: CLL1234 <- Read10X_h5(data)
```
Now you want to create the Seurat object from this count matrix.  A "Seurat object" is designed for scRNA-seq analysis that stores multiple types of data.
Types of data stored in this object: expression matrix (genes vs. cells), metadata(e.g., cell annotations), analysis results (e.g., clustering, dimensionality reduction).

```r
# Creating the Seurat object.
> C1234 <- CreateSeuratObject(counts = CLL1234, project = "CLL1234", min.cells = 3)
```
Consider if you have multiple samples in your project and you are looking at differences between timepoint 1 and timepoint 2.
If you have 5 samples in timepoint 1 and the same 5 samples in timepoint 2,
merge the files into TP1 and TP2 to do further analysis.

```r
TP1_merged <- merge(CLL111, 
                    y = list(CLL222, CLL333, CLL444, CLL555), 
                    add.cell.ids = c("CLL111", "CLL222", "CLL333", "CLL444", "CLL555"), 
                    project = "TP1")

# Merge Timepoint 2 samples into a single Seurat object
TP2_merged <- merge(CLL111_2, 
                    y = list(CLL222_2, CLL333_2, CLL444_2, CLL555_2), 
                    add.cell.ids = c("CLL111_2", "CLL222_2", "CLL333_2", "CLL444_2", "CLL555_2"), 
                    project = "TP2")

# Add timepoint metadata
TP1_merged$Timepoint <- "TP1"
TP2_merged$Timepoint <- "TP2"

# If you're planning to integrate or compare the two timepoints directly:

combined <- merge(TP1_merged, y = TP2_merged, add.cell.ids = c("TP1", "TP2"), project = "CLL_combined")

# You can proceed with integration, normalization, pca, clustering, etc.
```

```r
# Plug in "combined" when you see "CLL1234" (or whatever you name directory) if you are running merged timepoints.

An additional variable can be added when creating the Seurat object, known as, minimum features.  This will typically be 'min.features =200'.
Why would we have a "minimum features" filter?  We use "minimum features" because low-quality cells or empty droplets will often have very few genes (features).
We want to try to eliminate low quality cells. However, aberrantly high gene counts may be due to doublets and we can set the min.features <2,500. 
To include "minimum features" filter:
```r
C1234 <- CreateSeuratObject(counts = CLL1234, project = "CLL1234", min.cells = 3, min.features = 200)
# Then subset to keep cells with feature counts < 2,500
C1234 <- subset(C1234, subset = nFeature_RNA > 200 & nFeature_RNA < 2500)
```

Analyzing the amount of mitochondria in our sample.
We calculate mitochondrial QC metrics with the PercentageFeatureSet() function, which calculates the percentage of counts originating from a set of features.
We use the set of all genes starting with MT- as a set of mitochondrial genes.
```r
C1234[["percent.mt"]] <- PercentageFeatureSet(C1234, pattern = "^MT-")
# Now we want to visualize the features, the counts and the percent of mitochondria.
VlnPlot(C1234, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)

# FeatureScatter is typically used to visualize feature-feature relationships, but can be used
# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.
plot1 <- FeatureScatter(C1234, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(C1234, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2
```
Now we want to filter out cells that have > 10% mitochondrial counts.  This % will fluctuate depending on your stringency. Typical range is 5% to 20%.
```r
C1234 <- subset(C1234, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 10)
```

Normalize your data by a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.  
Please note that the normalization method below assumes that each cell originally contains the same number of RNA molecules.  
To avoid that assumption, use the SCTransform normalization function further below.

```r
C1234 <- NormalizeData(C1234)

#SCTransform method
SCTransform(
  C1234,
  assay = "RNA",
  new.assay.name = "SCT",
  reference.SCT.model = NULL,
  do.correct.umi = TRUE,
  ncells = 5000,
  residual.features = NULL,
  variable.features.n = 3000,
  variable.features.rv.th = 1.3,
  vars.to.regress = NULL,
  do.scale = FALSE,
  do.center = TRUE,
  clip.range = c(-sqrt(x = ncol(x = object[[assay]])/30), sqrt(x = ncol(x =
    object[[assay]])/30)),
  vst.flavor = "v2",
  conserve.memory = FALSE,
  return.only.var.genes = TRUE,
  seed.use = 1448145,
  verbose = TRUE,
  ...
)
```
Revisiting merged samples for timepoint analysis:
```r
# After you finish QC on combined datasets run the following:
combined <- NormalizeData(combined)
combined <- FindVariableFeatures(combined)
combined <- ScaleData(combined)
combined <- RunPCA(combined)
combined <- RunUMAP(combined, dims = 1:20)
combined <- FindNeighbors(combined, dims = 1:20)
combined <- FindClusters(combined, resolution = 0.5)  # Adjust resolution as needed

# Then run FeaturePlot to visualize marker expressions to identify the cluster of interest:
FeaturePlot(combined, features = c("NKG7", "GNLY"))
# Add cluster identities as metadata
combined$cluster <- Idents(combined)

# Create a table of counts, below assumes it was cluster 3 but whatever cluster you identify...
table(combined$Timepoint[combined$cluster == 3])

combined@meta.data %>%
  filter(cluster == 3) %>%
  count(Timepoint)

# To visualize the proportion of TP1 vs TP2 cells in each cluster:
library(ggplot2)

cluster_time_counts <- combined@meta.data %>%
  group_by(cluster, Timepoint) %>%
  summarise(n = n()) %>%
  mutate(percent = n / sum(n) * 100)

ggplot(cluster_time_counts, aes(x = cluster, y = percent, fill = Timepoint)) +
  geom_bar(stat = "identity", position = "fill") +
  ylab("Proportion of Cells") +
  theme_minimal()
```

For an actual cell count of a particular type of cell by timepoint, run the following:

```r
# Tag clusters
combined$cluster <- Idents(combined)

# Filter metadata for NK cluster and count by Timepoint
nk_counts <- table(combined$Timepoint[combined$cluster == 3])

# View result
nk_counts
```
However, if the particular type of cells you are looking at are spread across multiple clusters
or they're just not cleanly clustered (they usually aren't), then instead identify them 
using canonical marker expression:
```r
# Example: tag cells expressing NK markers like NKG7 and GNLY
nk_cells <- WhichCells(combined, expression = NKG7 > 1 & GNLY > 1)

# Subset metadata for these cells
nk_meta <- combined@meta.data[nk_cells, ]

# Count by Timepoint
table(nk_meta$Timepoint)

# This approach ensures you're counting cells based on biology, not just clustering artifacts.
```

Next we want to find highly variable features that can help us focus on these specific genes for biological insights. 
```r
C1234 <- FindVariableFeatures(C1234, selection.method = "vst", nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(VariableFeatures(C1234), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(C1234)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2
```
Scaling the data is a linear transformation and standard in pre-processing prior to PCA.
Scaling:
  Shifts the expression of each gene, mean expression across cells is 0.
  Scales the expression of each gene, variance across cells is 1.
  Equal wight in downstream analyses, so that highly -expressed genes do not dominate
Only variable features are scaled.
You can specify the features argument to scale addition features:
```r
all.genes <-rownames(C1234)
C1234 <- ScaleData(C1234, features = all.genes)
```
Linear Dimensional Reduction with Principle Component Analysis (PCA).
First, what is linear dimensional reduction?
  It is a linear algebra mathematical technique used to transform high-dimensional data into a lower-dimensional space.
  It does this by typically finding a new set of axes (the principle components in PCA).
  These new axes are linear combinations of the original features.
  Purpose is to reduce complexity while aiming to retain key patterns or variance in the data. 
PCA is used to reduce the number of features (genes or "dimensions") in the dataset while preserving as much information as possible.
When using Seurat and applying PCA to the scaled data, the first principal components will output a list of genes with the most positive and ngative loadings.
This will represent the modules of genes that exhibit either correlation (or anti-correlation) across single-cells in the data.
```r
C1234 <- RunPCA(C1234, features = VariableFeatures(object = C1234))
#Visualize the PCA results
print(C1234[["pca"]], dims = 1:5, nfeatures = 5)
```
```r
VizDimLoading(C1234, dims = 1:2, reduction = "pca")
```
```r
DimPlot(C1234, reduction = "pca") + NoLegend()
```
```r
DimHeatmap(C1234, dims = 1, cells = 500, balanced = TRUE)
```
```r
ElbowPlot(C1234)
```
Cluster the cells.
Seurat offers a graph-based clustering, using the distance metric to drive the clustering analysis.
Partitioning the cellular distance matrix into clusters by applying graph-based clustering appoaches.
The method embeds the cells into a graph structure (K-nearest neighbor (KNN)) using the euclidean distance formula,
with edges drawn between cells with similar feature expression patterns,
it then attempts to partition this graph into highly interconnected "communities" or "clusters".
To cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM
to iteratively group cells together.
The 'FindClusters()' function implements this procedure with a resolution parameter that sets the
granularity of the downstream clustering, with an increased number leading to increased clusters.
Set the parameter between 0.4-1.2 for good results for scRNA data that has ~ 3K cells.
Clusters can be found using Idents() function.
```r
C1234 <- FindNeighbors(C1234, dims = 1:10)
C1234 <- FindClusters(C1234, resolution = 0.5)
# Look at cluster IDs of the first 5 cells
head(Idents(C1234), 5)
```
Now run non-linear dimensional reduction (UMAP/tSNE) on your dataset.
The algorithm here shows underlying structure in the data, in order to place similar cells together.
Cells grouped together within graph-based clusters determined from above should co-localize
on the dimension reduction plots.
```r
C1234 <- RunUMAP(C1234c, dims = 1:10)
# note that you can set `label = TRUE` or use the LabelClusters function to help label
# individual clusters
DimPlot(C1234, reduction = "umap")

# find markers for every cluster compared to all remaining cells, report only the positive
# ones
C1234.markers <- FindAllMarkers(C1234, only.pos = TRUE)
C1234.markers %>%
    group_by(cluster) %>%
    dplyr::filter(avg_log2FC > 1)

```
Below you can find markers that are just specific to partiuclar clusters of interest.
For example, if you find that cluster 2 is the NK population you want to dive into:
```r
# find all markers of cluster 2
cluster2.markers <- FindMarkers(C1234, ident.1 = 2)
head(cluster2.markers, n = 5)

# Or you want to find all markers that distinguish cluster 2 from clusters 0 and 1
cluster2.markers <- FindMarkers(C1234, ident.1 = 2, ident.2 = c(0, 1))
head(cluster2.markers, n = 5)
```
Make violin plots of features of interest.
```r
VlnPlot(C1234, features = c("MS4A1", "CD79A"))

# Or make FeaturePlots of the various features of interest.
# This is helpful when trying to identify the clusters in your data.

FeaturePlot(C1234, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP",
    "CD8A"))
```
Now lets generate an expression heatmap of the top 20 markers for each cluster:
```r
C1234.markers %>%
    group_by(cluster) %>%
    dplyr::filter(avg_log2FC > 1) %>%
    slice_head(n = 10) %>%
    ungroup() -> top10
DoHeatmap(C1234, features = top10$gene) + NoLegend()
```

Now we are transitioning to the annotation of your clusters using scType.
QC, normalization, pre-processing, scaling, dimensional reduction and clustering have all been completed.
We want to identify clusters and then do population analysis on them.








